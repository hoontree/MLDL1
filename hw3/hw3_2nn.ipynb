{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPEoabX-hGCh"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMB3Lb9Lrry9",
        "outputId": "03addc6b-8241-44b0-fcac-0a46ed8daf6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6N3addYrAl3",
        "outputId": "46119223-cbd3-46a5-b861-731d7c38f6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OyammZP8hI7P"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "from mnist.data_utils import load_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxTNOvI5NHD"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xuQB6W2U5ZE2"
      },
      "outputs": [],
      "source": [
        "def leaky_relu(z, alpha=0.01):\n",
        "    \"\"\"\n",
        "    Implement the leaky relu activation function.\n",
        "    The method takes the input z and returns the output of the function.\n",
        "    Please DO NOT MODIFY the alpha value.\n",
        "\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    return np.where(z>0, z, alpha*z)\n",
        "\n",
        "\n",
        "def softmax(X):\n",
        "    \"\"\"\n",
        "    Implement the softmax function.\n",
        "    The method takes the input X and returns the output of the function.\n",
        "\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    exps = np.exp(X)\n",
        "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def deriv_leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"\n",
        "    Implement the derivative of leaky relu activation function.\n",
        "    The method takes the input z and returns the output of the function.\n",
        "    Please DO NOT MODIFY the alpha value.\n",
        "\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    return np.where(x>0, 1, alpha)\n",
        "\n",
        "def load_batch(X, Y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates batches with the remainder dropped.\n",
        "\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        permutation = np.random.permutation(X.shape[0])\n",
        "        X = X[permutation, :]\n",
        "        Y = Y[permutation, :]\n",
        "    num_steps = int(X.shape[0])//batch_size\n",
        "    step = 0\n",
        "    while step<num_steps:\n",
        "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
        "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
        "        step+=1\n",
        "        yield X_batch, Y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsU8v_6khR30"
      },
      "source": [
        "# 2-Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mA5udiGmhRb5"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNN:\n",
        "    \"\"\" a neural network with 2 layers \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        Do NOT modify this function.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_classes = num_classes\n",
        "        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n",
        "\n",
        "    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        initializes parameters with Xavier Initialization.\n",
        "\n",
        "        Question (b)\n",
        "        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization\n",
        "\n",
        "        Inputs\n",
        "        - input_dim\n",
        "        - num_hiddens\n",
        "        - num_classes\n",
        "        Returns\n",
        "        - params: a dictionary with the initialized parameters.\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        W1 = np.random.randn(input_dim, num_hiddens)/np.sqrt(input_dim)\n",
        "        W2 = np.random.randn(num_hiddens, num_classes)/np.sqrt(num_hiddens)\n",
        "        b1 = np.zeros((num_hiddens,))\n",
        "        b2 = np.zeros((num_classes,))\n",
        "\n",
        "        params['W1'] = W1\n",
        "        params['W2'] = W2\n",
        "        params['b1'] = b1\n",
        "        params['b2'] = b2\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Define and perform the feed forward step of a two-layer neural network.\n",
        "        Specifically, the network structue is given by\n",
        "\n",
        "          y = softmax(LeakyReLU(X W1 + b1) W2 + b2)\n",
        "\n",
        "        where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
        "        of shape (N, C), N is the number of examples (either the entire dataset or\n",
        "        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
        "\n",
        "        Question (c)\n",
        "        - ff_dict will be used to run backpropagation in backward method.\n",
        "\n",
        "        Inputs\n",
        "        - X: the input matrix of shape (N, D)\n",
        "\n",
        "        Returns\n",
        "        - y: the output of the model\n",
        "        - ff_dict: a dictionary with all the fully connected units and activations.\n",
        "        \"\"\"\n",
        "        ff_dict = {}\n",
        "        z1 = np.dot(X, self.params['W1']) + self.params['b1'] # (N, num_hidden) or (B, num_hidden)\n",
        "        a1 = leaky_relu(z1) # (N, num_hidden) or (B, num_hidden)\n",
        "        ff_dict['z1'], ff_dict['a1'] = z1,a1\n",
        "\n",
        "        z2 = np.dot(a1, self.params['W2']) + self.params['b2'] # (N, num_classes) or (B, num_classes)\n",
        "        y = softmax(z2) # (N, num_classes) or (B, num_classes)\n",
        "\n",
        "        ff_dict['z2'], ff_dict['y'] = z2, y\n",
        "        return y, ff_dict\n",
        "\n",
        "    def backward(self, X, Y, ff_dict):\n",
        "        \"\"\"\n",
        "        Performs backpropagation over the two-layer neural network, and returns\n",
        "        a dictionary of gradients of all model parameters.\n",
        "\n",
        "        Question (d)\n",
        "\n",
        "        Inputs:\n",
        "         - X: the input matrix of shape (B, D), where B is the number of examples\n",
        "              in a mini-batch, D is the feature dimensionality.\n",
        "         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
        "              where B is the number of examples in a mini-batch, C is the number\n",
        "              of classes.\n",
        "         - ff_dict: the dictionary containing all the fully connected units and\n",
        "              activations.\n",
        "\n",
        "        Returns:\n",
        "         - grads: a dictionary containing the gradients of corresponding weights and biases.\n",
        "        \"\"\"\n",
        "        grads = {}\n",
        "        dz2 = (ff_dict['y'] - Y)  # (N,num_classes)\n",
        "        grads['W2'] = np.dot(ff_dict['a1'].T, dz2) # (N,num_hidden)^T@(N,num_classes) = (num_classes,num_hidden)\n",
        "        grads['b2'] = np.sum(dz2, axis=0)  # (num_classes,)\n",
        "        dz1 = np.dot(dz2, self.params['W2'].T)*deriv_leaky_relu(ff_dict['z1']) # (N, num_classes)@(num_hidden, num_classes)^T * (N, num_hidden) = (N,num_hidden)\n",
        "        grads['W1'] = np.dot(X.T, dz1) # (N, D)^T@(N,num_hidden) = (D, num_hidden)\n",
        "        grads['b1'] = np.sum(dz1, axis=0) # (num_hidden,)\n",
        "\n",
        "        return grads\n",
        "\n",
        "\n",
        "    def compute_loss(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes cross entropy loss.\n",
        "\n",
        "        Do NOT modify this function.\n",
        "\n",
        "        Inputs\n",
        "            Y:\n",
        "            Y_hat:\n",
        "        Returns\n",
        "            loss:\n",
        "        \"\"\"\n",
        "        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n",
        "        \"\"\"\n",
        "        Runs mini-batch gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X\n",
        "        - Y\n",
        "        - X_val\n",
        "        - Y_Val\n",
        "        - lr\n",
        "        - n_epochs\n",
        "        - batch_size\n",
        "        - log_interval\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "          for X_batch, Y_batch in load_batch(X, Y, batch_size):\n",
        "              self.train_step(X_batch, Y_batch, batch_size, lr)\n",
        "          if epoch % log_interval==0:\n",
        "              Y_hat, ff_dict = self.forward(X)\n",
        "              train_loss = self.compute_loss(Y, Y_hat)\n",
        "              train_acc = self.evaluate(Y, Y_hat)\n",
        "              Y_hat, ff_dict = self.forward(X_val)\n",
        "              valid_loss = self.compute_loss(Y_val, Y_hat)\n",
        "              valid_acc = self.evaluate(Y_val, Y_hat)\n",
        "              print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n",
        "                    format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "\n",
        "    def train_step(self, X_batch, Y_batch, batch_size, lr):\n",
        "        \"\"\"\n",
        "        Updates the parameters using gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X_batch\n",
        "        - Y_batch\n",
        "        - batch_size\n",
        "        - lr\n",
        "        \"\"\"\n",
        "        _, ff_dict = self.forward(X_batch)\n",
        "        grads = self.backward(X_batch, Y_batch, ff_dict)\n",
        "        self.params[\"W1\"] -= lr * grads[\"W1\"]/batch_size\n",
        "        self.params[\"b1\"] -= lr * grads[\"b1\"]/batch_size\n",
        "        self.params[\"W2\"] -= lr * grads[\"W2\"]/batch_size\n",
        "        self.params[\"b2\"] -= lr * grads[\"b2\"]/batch_size\n",
        "\n",
        "    def evaluate(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes classification accuracy.\n",
        "\n",
        "        Do NOT modify this function\n",
        "\n",
        "        Inputs\n",
        "        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n",
        "             where C is the number of classes.\n",
        "        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n",
        "             where C is the number of classes.\n",
        "\n",
        "        Returns\n",
        "            accuracy: the classification accuracy in float\n",
        "        \"\"\"\n",
        "        classes_pred = np.argmax(Y_hat, axis=1)\n",
        "        classes_gt = np.argmax(Y, axis=1)\n",
        "        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXM2lWhtDYC6"
      },
      "source": [
        "# Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48ooR6YIxYhC",
        "outputId": "ed15a6d6-0d85-4128-ffcf-aad3e6f807eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST data loaded:\n",
            "Training data shape: (60000, 784)\n",
            "Training labels shape: (60000, 10)\n",
            "Test data shape: (10000, 784)\n",
            "Test labels shape: (10000, 10)\n",
            "\n",
            "Set validation data aside\n",
            "Training data shape:  (48000, 784)\n",
            "Training labels shape:  (48000, 10)\n",
            "Validation data shape:  (12000, 784)\n",
            "Validation labels shape:  (12000, 10)\n"
          ]
        }
      ],
      "source": [
        "X_train, Y_train, X_test, Y_test = load_data()\n",
        "\n",
        "idxs = np.arange(len(X_train))\n",
        "np.random.shuffle(idxs)\n",
        "split_idx = int(np.ceil(len(idxs)*0.8))\n",
        "X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n",
        "X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n",
        "print()\n",
        "print('Set validation data aside')\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', Y_train.shape)\n",
        "print('Validation data shape: ', X_valid.shape)\n",
        "print('Validation labels shape: ', Y_valid.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzw-D4Zr5xoi"
      },
      "source": [
        "# Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlnC_rerHPaN"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Question (e)\n",
        "# Tune the hyperparameters with validation data,\n",
        "# and print the results by running the lines below.\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqVyP3agw__8"
      },
      "source": [
        "I do grid search to find the best hyperparameter. Due to the limitation of computing resources, I skimmed relatively small region of parameter space. The range of each parameter I set in this model is as follow.\n",
        "\n",
        "num_hiddens = [32,64,128]\n",
        "\n",
        "lr_lst = [1e-4,1e-2,1e-1]\n",
        "\n",
        "n_epochs_lst = [20,100,200]\n",
        "\n",
        "batch_size_lst = [32,64,128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TTCqVT4S0Tm5"
      },
      "outputs": [],
      "source": [
        "# model instantiation\n",
        "model = TwoLayerNN(input_dim=784, num_hiddens=128, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cWb6xg0NxOs",
        "outputId": "0d0d951e-6d5d-478b-8d23-0f6a8c04cc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 00 - train loss/acc: 0.321 0.909, valid loss/acc: 0.337 0.904\n",
            "epoch 01 - train loss/acc: 0.263 0.925, valid loss/acc: 0.281 0.921\n",
            "epoch 02 - train loss/acc: 0.224 0.938, valid loss/acc: 0.244 0.932\n",
            "epoch 03 - train loss/acc: 0.195 0.946, valid loss/acc: 0.218 0.939\n",
            "epoch 04 - train loss/acc: 0.172 0.952, valid loss/acc: 0.198 0.944\n",
            "epoch 05 - train loss/acc: 0.153 0.957, valid loss/acc: 0.180 0.949\n",
            "epoch 06 - train loss/acc: 0.138 0.961, valid loss/acc: 0.166 0.951\n",
            "epoch 07 - train loss/acc: 0.126 0.965, valid loss/acc: 0.158 0.955\n",
            "epoch 08 - train loss/acc: 0.117 0.968, valid loss/acc: 0.150 0.957\n",
            "epoch 09 - train loss/acc: 0.108 0.969, valid loss/acc: 0.142 0.959\n",
            "epoch 10 - train loss/acc: 0.100 0.973, valid loss/acc: 0.135 0.961\n",
            "epoch 11 - train loss/acc: 0.092 0.975, valid loss/acc: 0.129 0.963\n",
            "epoch 12 - train loss/acc: 0.086 0.977, valid loss/acc: 0.122 0.964\n",
            "epoch 13 - train loss/acc: 0.082 0.978, valid loss/acc: 0.123 0.964\n",
            "epoch 14 - train loss/acc: 0.075 0.980, valid loss/acc: 0.117 0.965\n",
            "epoch 15 - train loss/acc: 0.073 0.980, valid loss/acc: 0.116 0.965\n",
            "epoch 16 - train loss/acc: 0.068 0.982, valid loss/acc: 0.112 0.966\n",
            "epoch 17 - train loss/acc: 0.066 0.982, valid loss/acc: 0.110 0.968\n",
            "epoch 18 - train loss/acc: 0.062 0.983, valid loss/acc: 0.108 0.968\n",
            "epoch 19 - train loss/acc: 0.057 0.985, valid loss/acc: 0.104 0.970\n",
            "epoch 20 - train loss/acc: 0.057 0.985, valid loss/acc: 0.104 0.968\n",
            "epoch 21 - train loss/acc: 0.053 0.987, valid loss/acc: 0.102 0.969\n",
            "epoch 22 - train loss/acc: 0.050 0.988, valid loss/acc: 0.100 0.969\n",
            "epoch 23 - train loss/acc: 0.047 0.989, valid loss/acc: 0.099 0.970\n",
            "epoch 24 - train loss/acc: 0.046 0.989, valid loss/acc: 0.099 0.970\n",
            "epoch 25 - train loss/acc: 0.044 0.990, valid loss/acc: 0.097 0.972\n",
            "epoch 26 - train loss/acc: 0.042 0.989, valid loss/acc: 0.097 0.971\n",
            "epoch 27 - train loss/acc: 0.041 0.990, valid loss/acc: 0.099 0.971\n",
            "epoch 28 - train loss/acc: 0.041 0.990, valid loss/acc: 0.097 0.971\n",
            "epoch 29 - train loss/acc: 0.037 0.991, valid loss/acc: 0.094 0.971\n",
            "epoch 30 - train loss/acc: 0.035 0.993, valid loss/acc: 0.092 0.972\n",
            "epoch 31 - train loss/acc: 0.036 0.992, valid loss/acc: 0.095 0.972\n",
            "epoch 32 - train loss/acc: 0.033 0.993, valid loss/acc: 0.094 0.971\n",
            "epoch 33 - train loss/acc: 0.031 0.993, valid loss/acc: 0.092 0.972\n",
            "epoch 34 - train loss/acc: 0.030 0.994, valid loss/acc: 0.091 0.972\n",
            "epoch 35 - train loss/acc: 0.029 0.994, valid loss/acc: 0.091 0.973\n",
            "epoch 36 - train loss/acc: 0.028 0.994, valid loss/acc: 0.091 0.972\n",
            "epoch 37 - train loss/acc: 0.028 0.994, valid loss/acc: 0.093 0.972\n",
            "epoch 38 - train loss/acc: 0.026 0.995, valid loss/acc: 0.091 0.973\n",
            "epoch 39 - train loss/acc: 0.025 0.995, valid loss/acc: 0.089 0.973\n",
            "epoch 40 - train loss/acc: 0.024 0.996, valid loss/acc: 0.091 0.972\n",
            "epoch 41 - train loss/acc: 0.024 0.996, valid loss/acc: 0.092 0.972\n",
            "epoch 42 - train loss/acc: 0.024 0.996, valid loss/acc: 0.094 0.972\n",
            "epoch 43 - train loss/acc: 0.021 0.996, valid loss/acc: 0.091 0.974\n",
            "epoch 44 - train loss/acc: 0.021 0.997, valid loss/acc: 0.090 0.974\n",
            "epoch 45 - train loss/acc: 0.020 0.997, valid loss/acc: 0.091 0.973\n",
            "epoch 46 - train loss/acc: 0.020 0.997, valid loss/acc: 0.089 0.974\n",
            "epoch 47 - train loss/acc: 0.018 0.998, valid loss/acc: 0.089 0.974\n",
            "epoch 48 - train loss/acc: 0.018 0.997, valid loss/acc: 0.090 0.974\n",
            "epoch 49 - train loss/acc: 0.018 0.998, valid loss/acc: 0.091 0.974\n",
            "epoch 50 - train loss/acc: 0.017 0.998, valid loss/acc: 0.090 0.973\n",
            "epoch 51 - train loss/acc: 0.017 0.998, valid loss/acc: 0.090 0.974\n",
            "epoch 52 - train loss/acc: 0.016 0.998, valid loss/acc: 0.091 0.974\n",
            "epoch 53 - train loss/acc: 0.016 0.998, valid loss/acc: 0.091 0.974\n",
            "epoch 54 - train loss/acc: 0.015 0.999, valid loss/acc: 0.089 0.974\n",
            "epoch 55 - train loss/acc: 0.015 0.998, valid loss/acc: 0.091 0.974\n",
            "epoch 56 - train loss/acc: 0.015 0.999, valid loss/acc: 0.090 0.973\n",
            "epoch 57 - train loss/acc: 0.013 0.999, valid loss/acc: 0.089 0.975\n",
            "epoch 58 - train loss/acc: 0.013 0.999, valid loss/acc: 0.091 0.974\n",
            "epoch 59 - train loss/acc: 0.013 0.999, valid loss/acc: 0.090 0.974\n",
            "epoch 60 - train loss/acc: 0.013 0.999, valid loss/acc: 0.091 0.974\n",
            "epoch 61 - train loss/acc: 0.012 0.999, valid loss/acc: 0.091 0.974\n",
            "epoch 62 - train loss/acc: 0.012 0.999, valid loss/acc: 0.092 0.974\n",
            "epoch 63 - train loss/acc: 0.012 0.999, valid loss/acc: 0.092 0.974\n",
            "epoch 64 - train loss/acc: 0.011 0.999, valid loss/acc: 0.091 0.974\n",
            "epoch 65 - train loss/acc: 0.011 0.999, valid loss/acc: 0.092 0.974\n",
            "epoch 66 - train loss/acc: 0.011 0.999, valid loss/acc: 0.091 0.974\n",
            "epoch 67 - train loss/acc: 0.011 0.999, valid loss/acc: 0.093 0.975\n",
            "epoch 68 - train loss/acc: 0.010 0.999, valid loss/acc: 0.092 0.974\n",
            "epoch 69 - train loss/acc: 0.010 0.999, valid loss/acc: 0.091 0.974\n",
            "epoch 70 - train loss/acc: 0.010 0.999, valid loss/acc: 0.092 0.975\n",
            "epoch 71 - train loss/acc: 0.009 0.999, valid loss/acc: 0.092 0.974\n",
            "epoch 72 - train loss/acc: 0.009 1.000, valid loss/acc: 0.091 0.975\n",
            "epoch 73 - train loss/acc: 0.009 0.999, valid loss/acc: 0.094 0.975\n",
            "epoch 74 - train loss/acc: 0.009 0.999, valid loss/acc: 0.092 0.975\n",
            "epoch 75 - train loss/acc: 0.008 1.000, valid loss/acc: 0.092 0.974\n",
            "epoch 76 - train loss/acc: 0.008 1.000, valid loss/acc: 0.092 0.974\n",
            "epoch 77 - train loss/acc: 0.008 1.000, valid loss/acc: 0.092 0.975\n",
            "epoch 78 - train loss/acc: 0.008 1.000, valid loss/acc: 0.092 0.975\n",
            "epoch 79 - train loss/acc: 0.008 1.000, valid loss/acc: 0.093 0.975\n",
            "epoch 80 - train loss/acc: 0.007 1.000, valid loss/acc: 0.093 0.975\n",
            "epoch 81 - train loss/acc: 0.007 1.000, valid loss/acc: 0.094 0.975\n",
            "epoch 82 - train loss/acc: 0.007 1.000, valid loss/acc: 0.093 0.974\n",
            "epoch 83 - train loss/acc: 0.007 1.000, valid loss/acc: 0.093 0.975\n",
            "epoch 84 - train loss/acc: 0.007 1.000, valid loss/acc: 0.093 0.975\n",
            "epoch 85 - train loss/acc: 0.007 1.000, valid loss/acc: 0.094 0.975\n",
            "epoch 86 - train loss/acc: 0.007 1.000, valid loss/acc: 0.093 0.975\n",
            "epoch 87 - train loss/acc: 0.006 1.000, valid loss/acc: 0.094 0.975\n",
            "epoch 88 - train loss/acc: 0.006 1.000, valid loss/acc: 0.094 0.975\n",
            "epoch 89 - train loss/acc: 0.006 1.000, valid loss/acc: 0.093 0.975\n",
            "epoch 90 - train loss/acc: 0.006 1.000, valid loss/acc: 0.094 0.975\n",
            "epoch 91 - train loss/acc: 0.006 1.000, valid loss/acc: 0.094 0.975\n",
            "epoch 92 - train loss/acc: 0.006 1.000, valid loss/acc: 0.094 0.975\n",
            "epoch 93 - train loss/acc: 0.006 1.000, valid loss/acc: 0.095 0.975\n",
            "epoch 94 - train loss/acc: 0.006 1.000, valid loss/acc: 0.095 0.976\n",
            "epoch 95 - train loss/acc: 0.006 1.000, valid loss/acc: 0.095 0.975\n",
            "epoch 96 - train loss/acc: 0.005 1.000, valid loss/acc: 0.095 0.974\n",
            "epoch 97 - train loss/acc: 0.005 1.000, valid loss/acc: 0.095 0.974\n",
            "epoch 98 - train loss/acc: 0.005 1.000, valid loss/acc: 0.097 0.975\n",
            "epoch 99 - train loss/acc: 0.005 1.000, valid loss/acc: 0.095 0.975\n",
            "epoch 100 - train loss/acc: 0.005 1.000, valid loss/acc: 0.095 0.975\n",
            "epoch 101 - train loss/acc: 0.005 1.000, valid loss/acc: 0.096 0.975\n",
            "epoch 102 - train loss/acc: 0.005 1.000, valid loss/acc: 0.096 0.975\n",
            "epoch 103 - train loss/acc: 0.005 1.000, valid loss/acc: 0.096 0.975\n",
            "epoch 104 - train loss/acc: 0.005 1.000, valid loss/acc: 0.096 0.975\n",
            "epoch 105 - train loss/acc: 0.005 1.000, valid loss/acc: 0.096 0.975\n",
            "epoch 106 - train loss/acc: 0.004 1.000, valid loss/acc: 0.096 0.975\n",
            "epoch 107 - train loss/acc: 0.004 1.000, valid loss/acc: 0.096 0.975\n",
            "epoch 108 - train loss/acc: 0.004 1.000, valid loss/acc: 0.097 0.975\n",
            "epoch 109 - train loss/acc: 0.004 1.000, valid loss/acc: 0.096 0.975\n",
            "epoch 110 - train loss/acc: 0.004 1.000, valid loss/acc: 0.097 0.975\n",
            "epoch 111 - train loss/acc: 0.004 1.000, valid loss/acc: 0.097 0.975\n",
            "epoch 112 - train loss/acc: 0.004 1.000, valid loss/acc: 0.097 0.975\n",
            "epoch 113 - train loss/acc: 0.004 1.000, valid loss/acc: 0.097 0.975\n",
            "epoch 114 - train loss/acc: 0.004 1.000, valid loss/acc: 0.097 0.975\n",
            "epoch 115 - train loss/acc: 0.004 1.000, valid loss/acc: 0.098 0.975\n",
            "epoch 116 - train loss/acc: 0.004 1.000, valid loss/acc: 0.098 0.975\n",
            "epoch 117 - train loss/acc: 0.004 1.000, valid loss/acc: 0.097 0.976\n",
            "epoch 118 - train loss/acc: 0.004 1.000, valid loss/acc: 0.098 0.975\n",
            "epoch 119 - train loss/acc: 0.004 1.000, valid loss/acc: 0.097 0.975\n",
            "epoch 120 - train loss/acc: 0.004 1.000, valid loss/acc: 0.099 0.974\n",
            "epoch 121 - train loss/acc: 0.004 1.000, valid loss/acc: 0.098 0.976\n",
            "epoch 122 - train loss/acc: 0.003 1.000, valid loss/acc: 0.098 0.976\n",
            "epoch 123 - train loss/acc: 0.003 1.000, valid loss/acc: 0.098 0.975\n",
            "epoch 124 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.975\n",
            "epoch 125 - train loss/acc: 0.003 1.000, valid loss/acc: 0.098 0.975\n",
            "epoch 126 - train loss/acc: 0.003 1.000, valid loss/acc: 0.098 0.975\n",
            "epoch 127 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.975\n",
            "epoch 128 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.975\n",
            "epoch 129 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.975\n",
            "epoch 130 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.975\n",
            "epoch 131 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.975\n",
            "epoch 132 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.975\n",
            "epoch 133 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.976\n",
            "epoch 134 - train loss/acc: 0.003 1.000, valid loss/acc: 0.099 0.975\n",
            "epoch 135 - train loss/acc: 0.003 1.000, valid loss/acc: 0.100 0.975\n",
            "epoch 136 - train loss/acc: 0.003 1.000, valid loss/acc: 0.100 0.975\n",
            "epoch 137 - train loss/acc: 0.003 1.000, valid loss/acc: 0.100 0.976\n",
            "epoch 138 - train loss/acc: 0.003 1.000, valid loss/acc: 0.100 0.976\n",
            "epoch 139 - train loss/acc: 0.003 1.000, valid loss/acc: 0.100 0.975\n",
            "epoch 140 - train loss/acc: 0.003 1.000, valid loss/acc: 0.100 0.975\n",
            "epoch 141 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 142 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 143 - train loss/acc: 0.003 1.000, valid loss/acc: 0.100 0.975\n",
            "epoch 144 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 145 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 146 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 147 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 148 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 149 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 150 - train loss/acc: 0.003 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 151 - train loss/acc: 0.002 1.000, valid loss/acc: 0.101 0.976\n",
            "epoch 152 - train loss/acc: 0.002 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 153 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 154 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.976\n",
            "epoch 155 - train loss/acc: 0.002 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 156 - train loss/acc: 0.002 1.000, valid loss/acc: 0.101 0.975\n",
            "epoch 157 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 158 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 159 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 160 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 161 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 162 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 163 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 164 - train loss/acc: 0.002 1.000, valid loss/acc: 0.103 0.975\n",
            "epoch 165 - train loss/acc: 0.002 1.000, valid loss/acc: 0.103 0.975\n",
            "epoch 166 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 167 - train loss/acc: 0.002 1.000, valid loss/acc: 0.102 0.975\n",
            "epoch 168 - train loss/acc: 0.002 1.000, valid loss/acc: 0.103 0.976\n",
            "epoch 169 - train loss/acc: 0.002 1.000, valid loss/acc: 0.103 0.975\n",
            "epoch 170 - train loss/acc: 0.002 1.000, valid loss/acc: 0.103 0.975\n",
            "epoch 171 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 172 - train loss/acc: 0.002 1.000, valid loss/acc: 0.103 0.975\n",
            "epoch 173 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 174 - train loss/acc: 0.002 1.000, valid loss/acc: 0.103 0.975\n",
            "epoch 175 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 176 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 177 - train loss/acc: 0.002 1.000, valid loss/acc: 0.103 0.976\n",
            "epoch 178 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.976\n",
            "epoch 179 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 180 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 181 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 182 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 183 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.976\n",
            "epoch 184 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 185 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 186 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 187 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 188 - train loss/acc: 0.002 1.000, valid loss/acc: 0.104 0.975\n",
            "epoch 189 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 190 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 191 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 192 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.976\n",
            "epoch 193 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 194 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 195 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 196 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 197 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.975\n",
            "epoch 198 - train loss/acc: 0.002 1.000, valid loss/acc: 0.105 0.976\n",
            "epoch 199 - train loss/acc: 0.002 1.000, valid loss/acc: 0.106 0.976\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "lr, n_epochs, batch_size = 0.1, 200, 128\n",
        "model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpPsAlXU0T_Z",
        "outputId": "2ccf5be5-e6a9-4f61-c74a-4921a52e7315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test loss = 0.089, acc = 0.978\n"
          ]
        }
      ],
      "source": [
        "# evalute the model on test data\n",
        "Y_hat, _ = model.forward(X_test)\n",
        "test_loss = model.compute_loss(Y_test, Y_hat)\n",
        "test_acc = model.evaluate(Y_test, Y_hat)\n",
        "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5J0b4We2zG3q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}